\documentclass[11pt]{article}
\usepackage{../acl2016}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here
%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\usepackage[breaklinks, colorlinks, linkcolor=black, urlcolor=black, citecolor=black, draft]{hyperref}
\usepackage{natbib}
\usepackage{times}
\usepackage{latexsym}
% \setlength\titlebox{7.5cm}    % Expanding the titlebox

%%% Custom additions %%%
\usepackage{url}
\usepackage[leqno, fleqn]{amsmath}
\usepackage{amssymb}
\usepackage{qtree}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{tikz}
\usepackage{tikz-qtree}
\usepackage{pgfplots}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{stmaryrd}
\usepackage{gb4e}

\newcommand\todo[1]{\textcolor{blue}{\textbf{TODO:} #1}}
\newcommand\result[1]{\textcolor{red}{\textbf{RESULT NEEDED:} #1}}
\newcommand\question[1]{\textcolor{orange}{\textbf{OPEN QUESTION:} #1}}

\newcount\colveccount
\newcommand*\colvec[1]{
        \global\colveccount#1
        \begin{bmatrix}
        \colvecnext
}
\def\colvecnext#1{
        #1
        \global\advance\colveccount-1
        \ifnum\colveccount>0
                \\
                \expandafter\colvecnext
        \else
                \end{bmatrix}
        \fi
}

\newcommand{\shift}{\textsc{shift}}
\newcommand{\reduce}{\textsc{reduce}}
 
\def\ii#1{\textit{#1}}
\newcommand{\word}[1]{\emph{#1}}
\newcommand{\fulllabel}[2]{\b{#1}\newline\textsc{#2}}

\noautomath

\title{A fast unified model for parsing and sentence understanding} 

     \author{}
%    \author{
%    Samuel R.\ Bowman$^{1,2,5,}$\thanks{~\,The first two authors contributed equally.} \\
%    \texttt{\small sbowman@stanford.edu} \\
%    \And
%    Jon Gauthier$^{2,3,5,*}$ \\
%    \texttt{\small jgauthie@stanford.edu} \\
%    \And
%    Abhinav Rastogi$^{4,5}$ \\
%    \texttt{\small arastogi@stanford.edu} \\
%    \AND
%    Raghav Gupta$^{6}$ \\
%    \texttt{\small rgupta93@stanford.edu} \\
%    \And
%    Christopher D.\ Manning$^{1,2,5,6}$\\
%    \texttt{\small manning@stanford.edu}\\
%    \And
%    Christopher Potts$^{1}$\\
%    \texttt{\small cgpotts@stanford.edu}
%    \AND\\[-3ex]
%    {$^{1}$Stanford Linguistics\quad
%    $^{2}$Stanford NLP Group\quad
%    $^{3}$Stanford Symbolic Systems}\\
%    {$^{4}$Stanford Electrical Engineering\quad
%    $^{5}$Stanford AI Lab\quad
%    $^{6}$Stanford Computer Science}
%    }

\date{}


\begin{document}
\maketitle
\begin{abstract}

Tree-structured neural networks exploit valuable syntactic parse information as they interpret the meanings of sentences. However, they suffer from two key technical problems that make them slow and unwieldy for large-scale NLP tasks: they can only operate on parsed sentences and do not directly support batched computation. We address these issues by introducing the Stack-augmented Parser-Interpreter Neural Network (SPINN), which combines parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shift-reduce parser. Our model supports batched computation for a speedup of up to 20x over other tree-structured models, and its integrated parser allows it to operate on unparsed data with nearly no loss of accuracy. We evaluate it on the Stanford NLI entailment task and show that it significantly outperforms other sentence encoding models.
\end{abstract}

\section{Introduction}

\input{batching_fig.tex}
\input{../model1_fig.tex}

A wide range of current state-of-the-art models in NLP are built around neural network components that build vector representations of sentence meaning \citep{socher2011semi,sutskever2014sequence}. This component, the sentence encoder, is generally formulated as a learned parametric function from a sequence of word vector to a sentence vector, and this function can take a range of different forms. Common sentence encoders include sequence-based recurrent neural network models (RNNs) with Long Short-Term Memory \citep[LSTM,][see Figure~\ref{fig:batching:good}]{hochreiter1997long}, which accumulate information over the sentence sequentially, convolutional neural networks over words or characters \citep{kalchbrenner2014convolutional,DBLP:journals/corr/ZhangZL15}, which accumulate information using filters over short local sequences of words, and tree-structured recursive neural networks \citep[TreeRNNs,][see Figure~\ref{fig:batching:bad}]{goller1996learning,socher2011semi}, which propagate information up a binary parse tree structure over words. 

Of these, it is easy to argue that the TreeRNN is the principled choice, since meaning in natural language sentences constructed incrementally according to a tree structure \citep{Partee84,Janssen97}. However, while these models have shown promise \citep{tai2015improved,li2015tree,bowman2015trees}, they have largely been overlooked in favor of sequence-based RNNs because of their incompatibility with batched computation and their reliance on external parsers.  Batched computation---performing synchronized computation across many examples at once---yields order-of-magnitude improvements in model run time, and is a crucial enabling technology in allowing neural networks to be trained efficiently on large datasets. Because TreeRNNs use a different model structure for each sentence, batched computation is impossible in standard implementations. In addition, standard TreeRNN models can only operate on sentences that have already been processed by a syntactic parser, which slows and complicates the implementation of these models for most applications.

This paper introduces a new model to address both these issues: the Stack-augmented Parser-Interpreter Neural Network, or SPINN, shown in Figure~\ref{fig:m1-views}. The SPINN executes the computations of a tree-structured model in a linearized sequence of operations, and can incorporate a neural network parser that determines the overall tree structure on-the-fly. This design improves upon the TreeRNN architecture in three ways:
\begin{itemize}
\item At test time, it can simultaneously parse and interpret unparsed sentences, removing the dependence on an external parser.
\item It supports batched computation for both parsed and unparsed sentences, supporting dramatic speedups over standard TreeRNNs.
\item It supports a novel tree-sequence hybrid mechanism for handling local context in sentence interpretation that yields substantial gains in accuracy over either pure sequence-based models or pure tree-based models.
\end{itemize}

We evaluate the SPINN on the Stanford Natural Language Inference sentence understanding task \citep[SNLI,][]{snli:emnlp2015}, and find that it significantly outperforms other sentence encoding-based models, and that it yields speed increases of upwards of 10x over comparable TreeRNN models.

\subsection{Related work}

There has been a fairly long history of work on building neural-network based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a version \citep{henderson2004discriminative,emami2005neural,titov2010latent,chen2014,buys2generative,dyer-EtAl:2015:ACL-IJCNLP,kiperwasser2016easy}. In addition, there has been recent work \citep{zhang2016top,dyer2016rnn} proposing models designed primarily for generative language modeling tasks that use these structures as well. To our knowledge, the SPINN is the first model to use these structures for the purpose of sentence interpretation, rather than parsing or generation.

\citet{socher2011dynamic} presents a version of the TreeRNN model which is capable of operating over unparsed inputs. However, this method is unsupervised, limiting its performance, and it requires an expensive search process at test time. Our model presents an alternative approach to combining interpretation and parsing that supports supervised training for the parsing component, and that is dramatically faster than any previous tree-structured model. 

\section{Our model: The SPINN}

\subsection{Shift-reduce parsing}

The SPINN is inspired by the shift-reduce parsing formalism \citep{aho1972theory}, which builds a tree structure over a sequence (e.g. a natural language sentence) by a single left-to-right scan over the tokens of the sequence. Shift-reduce parsing is frequently used in natural language parsing \citep[e.g.][]{nivre2003efficient}.

A shift-reduce parser accepts a sequence of input tokens $\mathbf x = (x_1, \dots, x_N)$ and transitions $\mathbf t = (t_1, \dots, t_T)$, where each $t_i \in \{\shift, \reduce\}$ specifies a \shift~or \reduce~transition (described below). In general a parser may also generate these predictions on-the-fly as it reads the tokens. It reads left-to-right over the input transition sequence, combining the input tokens $\mathbf x$ incrementally into a tree structure. For any binary-branching tree structure over $N$ words, this requires $T = 2N - 1$ transitions.

The parser uses two auxiliary data structures, a stack $S$ of partially completed subtrees and a buffer $B$ of tokens yet to be parsed. We initialize the parser with an empty stack and with a buffer containing all the sentence tokens $\mathbf x$. Let $\langle S, B \rangle = \langle \emptyset, \mathbf x \rangle$ denote this starting state. We next proceed through the transition sequence, where each transition $t_i$ selects one of the two following operations. Below the $\mid$ symbol denotes the \textit{cons} (concatenation) operator. We arbitrarily choose to always \textit{cons} on the left in the notation below.
\begin{description}
  \item[\shift:] $\langle S, x \mid B \rangle \to \langle x \mid S, B \rangle$. This operation pops an element from the buffer and pushes it onto the top of the stack.
  \item[\reduce:] $\langle x \mid y \mid S, B \rangle \to \langle (x, y) \mid S, B \rangle$. This operation pops the top two elements from the stack, merges them into a binary tree with children $(x, y)$, and pushes the result back onto the stack.
\end{description}

\subsection{Composition and representation}

The core SPINN model implements a sequential shift-reduce parser which operates over sentences. It is designed to produce a vector representation of the sentence as output, as opposed to a tree as in standard shift-reduce parsing. This relies on two key modifications to the shift-reduce algorithm:
\begin{itemize}
\item The parser's intermediate representations, including the representations of partial tree structures on the stack, are fixed-length vectors.
\item The \reduce~operation combines two such representations of trees on the stack into a larger tree using a version of the TreeLSTM neural network layer \citep{tai2015improved}.
\end{itemize}

This subsection describes the core functions and representations of the SPINN. We begin with the most basic model, SPINN-NP (for \textit{no parser}), which processes input sentences using parses provided by some external parser.

\paragraph{Word representations}

The buffer is populated with the words in each sentence before the first step of computation. We use word representations from the standard 300-dimensional vector package provided with GloVe \citep{pennington2014glove}. We do not update these representations during training. Instead, we use a learned linear transformation to transform the representation of each input word $\vec{x}$ into a pair of vectors $\langle \vec{h}, \vec{c}\rangle$ that are stored side-by-side in the buffer and used as inputs to the TreeLSTM composition function:

\begin{equation}
\colvec{2}
    {\vec{h}}
    {\vec{c}}
= W_{\text{wd}} \vec{x}
\end{equation}


\paragraph{The composition function}
When a \reduce~operation is performed, vector representations of two tree nodes are popped off of the head of the stack and fed into a {\ii composition function}, which is a neural network function that produces a representation for a new tree node that is the parent of the two popped nodes. This new node is then pushed on to the stack.

Our composition is a version of the TreeLSTM of \citet{tai2015improved}. The TreeLSTM layer, which generalizes the LSTM neural network layer to tree- rather than sequence-based inputs, and it shares with the LSTM the idea of representing intermediate states as a pair of a fast-changing state representation $\vec{h}$ and a slower-changing memory representation $\vec{c}$. It is formulated as:

\begin{gather}
\colvec{4}
    {\vec{i}}
    {\vec{f}_l}
    {\vec{f}_r}
    {\vec{o}}
= \sigma\left(
W_{\text{iffo}}
\colvec{3}
    {\vec{h}_s^1}
    {\vec{h}_s^2}
    {\vec{e}}
\right) \label{eqn:lstm1}
\\
\vec{g}
= \tanh\left(
W_{g}
\colvec{3}
    {\vec{h}_s^1}
    {\vec{h}_s^2}
    {\vec{e}}
\right) \label{eqn:lstm2}
\\
\vec{c} = \vec{f}_l \odot \vec{c}_s^{\,2} + \vec{f}_r \odot \vec{c}_s^{\,1} + \vec{i} \odot \vec{g}  
\\
\vec{h} = \vec{o} \odot \vec{c}
\end{gather}

The results of this function are the pair $\langle\vec{h}, \vec{c}\rangle$, which are placed back on the stack. The two input tree nodes popped off the stack are represented as the pairs $\langle\vec{h}^1_s, \vec{c}^{\,1}_s\rangle$ and $\langle\vec{h}^2_s, \vec{c}^{\,2}_s\rangle$. In addition, $\vec{e}$ is an optional input argument which is either the empty vector or a vector from an external source like the tracking LSTM (see Section~\ref{sec:tracking}). $\odot$ denotes the elementwise product. Each vector-valued variable listed here is of the same dimension $D$ except $\vec{e}$, which is of the independent dimension $D_t$.

\paragraph{The contents of the stack and buffer}

The stack and the buffer can be represented as lists of $N$ elements each (for sentences of up to $N$ words), with two $D$-dimensional vectors $\vec{h}$ and $\vec{c}$ constituting each element.

\paragraph{Creating a sentence pair classifier}

This paper evaluates the SPINN on a sentence pair classification task. To classify a sentence pair, a feature vector $\vec{x}_{\text{classifier}}$ is first constructed. This feature vector is based on the final representations of each of the two sentences---the representations that appear at the head of the stack for each sentence after the final transition. In particular, the $\vec{h}$ portions of these representations are used. The final feature vector consists of the concatenation of these two vectors, $\vec{h}_{\text{premise}}$ and $\vec{h}_{\text{hypothesis}}$, their difference, and their elementwise product:

\begin{equation}
\vec{x}_{\text{classifier}} = 
\colvec{4}
    {\vec{h}_{\text{premise}}}
    {\vec{h}_{\text{hypothesis}}}
    {\vec{h}_{\text{premise}} - \vec{h}_{\text{hypothesis}}}
    {\vec{h}_{\text{premise}} \odot \vec{h}_{\text{hypothesis}}}
\end{equation}

Following \citep{snli:emnlp2015}, this feature vector is then passed to a series of ReLU neural network layers (the number of layers is a tuned hyperparameter), then passed into a linear transformation, and then finally passed to a softmax layer, which yields a distribution among the three labels. The model is trained in part using a cross-entropy objective over this distribution.

\subsection{The tracking LSTM}\label{sec:tracking}

In addition to the stack, the buffer, and the composition function, our full model includes an additional component: the tracking LSTM. The tracking LSTM is a simple low-dimensional sequence-based LSTM RNN that operates in tandem with the model, taking inputs from the buffer and stack at each step. It is meant to maintain a low-resolution summary of the portion of the sentence that has been processed so far, and it is used for two purposes: it supplies feature representations to the transition decision function, which allows the model to stand alone as a parser, and it additionally supplies a secondary input $\vec{e}$ (see Equations~\ref{eqn:lstm1}--\ref{eqn:lstm2}) to the composition function, allowing context information to leak into the construction of sentence meaning, and forming what is effectively a tree-sequence hybrid model.

The tracking LSTM takes three vectors as inputs at each step (highlighted in yellow in Figure~\ref{fig:m1-views}): the top element of the buffer $\vec{h}_b^1$, which would be moved in a \shift~operation, and the top two elements of the stack $\vec{h}_s^1$ and $\vec{h}_s^2$ which would be composed in a \reduce~operation.

\paragraph{Tracking left context for parsing} The decision function which determines which transitions (and thereby, which parses) the SPINN will use needs to take as its input some representation of the current state of the model. We use the hidden state of the tracking LSTM as that input. The tracking LSTM's limited access to the buffer makes this fast to compute, but somewhat more limited than the rich feature sets used in state-of-the-art parsers.

\paragraph{Tracking left context for interpretation} Lexical ambiguity is ubiquitous in natural language. Most words have multiple senses or meanings, and it is generally necessary to use the context in which a word occurs to determine which of its senses or meanings is meant in a given sentence. Simpler sequence-based sentence encoding models like the standard LSTM RNN have an advantage here: when a sequence-based model first processes a word, it has direct access to a state vector that summarizes the left context of that word, which provides some cues for disambiguation. In contrast, when standard tree-structured models first process a word, they only have access to the constituent that the word is merging with, which is often just a single additional word. Feeding a context representation from the tracking LSTM into the composition function is a simple and efficient way to mitigate this disadvantage of tree-structured models. 

It would be straightforward to augment the SPINN to support the use of some amount of right context as well, but this would add complexity to the model that we think is largely unnecessary: humans are very effective at understanding the beginnings of sentences before having seen or heard the ends, suggesting that it is possible to get by without the unavailable right context.

\todo{[JG] Include figure of post-order traversal sequence made by the tracking LSTM?}

\subsection{Parsing: Predicting transitions from the tracking state}

The model described so far is the SPINN-NP model, which uses sequences of \shift~and \reduce~operations from an external parser. To build the fully independent SPINN model, we simply add a single-layer neural network classifier that chooses which operation to perform at each step using the state of the tracking LSTM:
\begin{equation}
\vec{p}_{\text{trans}} = \text{softmax}(W_{\text{trans}}\vec{h}_{\text{track}})
\end{equation}

The resulting vector $\vec{p}_{\text{trans}}$ is a probability distribution over parsing decisions (i.e., predicting whether to \shift~or \reduce~at the current timestep). In the full SPINN model, we follow whichever transition has a higher predicted probability in this distribution. This prediction module is trained to mimic the decisions of an external parser. In particular, we use the transition sequences corresponding to the constituency parses included in the SNLI corpus. The classifier is trained using a cross-entropy objective:
\begin{equation}
  L_{\text{trans}} = - \log \vec{p}_{\text{trans}}[t^*]
\end{equation}
where $t^*$ is the correct transition specified by the gold parse, and $\vec{p}[\cdot]$ represents vector indexing.

At training time, the model follows the transitions corresponding to the gold parse. At test time, a transition is chosen at each step by taking a hard max over the output of this function. We did not find the technique of scheduled sampling \citep{bengio2015scheduled}, or allowing the model to use its own transition decisions in some instances at training time, to be helpful.

\subsection{Implementing the SPINN}

\paragraph{The full objective function} Our objective function combines a cross-entropy objective $\mathcal{L}_{\text{s}}$ for the SNLI classification task, a cross-entropy objective $\{\mathcal{L}_p^t, \mathcal{L}_h^t\}$ for each parsing decision for each of the sentence and the hypothesis sentence at each step $t$, and an L2 regularization term on the trained parameters. The terms are weighted using the tuned hyperparameters $\alpha$ and $\lambda$:

\begin{equation}
\begin{split}
\mathcal{L}_{\text{m}} = &\mathcal{L}_{\text{s}} + \alpha \sum_{t=0}^{T-1} (\mathcal{L}_{\text{p}}^{t} + \mathcal{L}_{\text{h}}^{t}) + \lambda \|\theta\|^2_2
\end{split}
\end{equation}

\paragraph{Preparing the data} At training time, the SPINN requires both a transition sequences $\mathbf t$  and a token sequence $\mathbf x$ as its inputs for each sentence. The token sequence is simply the words in the sentence in order. $\mathbf t$ can be obtained from any constituency parse for the sentence by first converting that parse into an unlabeled binary parse, then linearizing it (with the usual in-order traversal), then taking each word token as a \shift~transition and each `)' as a \reduce~transition, as here:

\vspace{0.5em}
{\noindent\small
{\bf Unlabeled binary parse:} ( ( the cat ) ( sat down ) ) $\Rightarrow$\\
{$\mathbf t$}: \shift, \shift, \reduce, \shift, \shift, \reduce, \reduce\\
{$\mathbf x$}: the, cat, sat, down
}

\paragraph{Handling variable sentence lengths} For any sentence model to be trained with batched computation, it is necessary to pad or crop sentences to a fixed length. We fix this length at $N = 25$ words, longer than about 98\% of sentences in SNLI. Transition sequences $\mathbf t$ are cropped at the left or padded at the left with \shift s. Token sequences $\mathbf x$ are then cropped or padded with empty tokens at the left to match the number of \shift s added or removed from $\mathbf t$, and can then be padded with empty tokens at the right to meet the desired length $N$.

\paragraph{Optimization, initialization, and hyperparameters}

We use the RMSProp optimizer \citep{tieleman2012lecture} with a tuned starting learning rate that decays by a factor of 0.75 every 10k steps. We apply both dropout \citep{srivastava2014dropout} and batch normalization \citep{2015SIoffeCSzegedy} to the word embeddings in the buffer (after the linear projection is applied) and to the feature vectors that serve as the inputs and outputs to the MLP that precedes the final entailment classifier.

We initialize the model parameters using the nonparametric strategy of \citet{DBLP:journals/corr/HeZR015}, with the exception of the softmax classifier layers, which we initialize uniformly from $[-0.005, 0.005]$.

We use random search to tune the hyperparameters of the model, setting the ranges for search for each hyperparameter heuristically (and validating the reasonableness of the ranges on the development set), and than launching eight copies of each experiment each with newly sampled hyperparameters from those ranges. An appendix shows the precise values used on the strongest models.

We trained each model for 250k steps in each run, using a batch size of 32 for each step. We tracked each model's performance on the development set during training and saving parameters when this performance reached a new peak. We used early stopping, evaluating on the test set using the parameters that performed best on the development set.

\paragraph{Software infrastructure} We will make source code available.

\subsection{Efficient stack representation}

A naive implementation of the SPINN model would require simulating a stack of size $N$ for each input sentence. In order to update the model weights via backpropagation, we would also need to maintain all intermediate stack representations for later chain rule calculations. This implies a per-example space requirement of $N \times T \times D$, which is prohibitively large for significant batch sizes $M$ or sentence lengths $N$. (Recall that $T = 2N - 1$ is the number of transitions in the shift-reduce parsing sequence, and $D$ is the dimension of the vectors on the stack.) Such a naive implementation would also require duplicating a largely unchanged stack at each timestep (as \shift~and \reduce~operations only affect the top of the stack).

We designed an alternative minimal-space stack algorithm inspired by the zipper technique \citep{huet1997zipper}. For a single input sentence, we represent the stack with a single $T \times D$ matrix $S$. Each row $S_t$ represents the top of the actual stack at timestep $t$. We maintain a queue of backpointers onto $S$ in order to track which elements should be involved in a \reduce~operation at any given time. Algorithm~\ref{alg:thin-stack} below describes the full mechanics of a stack feedforward in this compressed representation. It operates on the compressed $T \times D$ matrix $S$ and a backpointer queue $Q$. Table~\ref{tbl:thin-stack} shows an example run of this algorithm.

\begin{algorithm}
\caption{The thin-stack algorithm}
\label{alg:thin-stack}
\begin{algorithmic}[1]
  \Function{Step}{bufferTop, op, $t$, $S$, $Q$}
    \If{op = \shift}
      \State $S$[$t$] := bufferTop
    \ElsIf{op = \reduce}
      \State right := $S$[$Q$.pop()]
      \State left := $S$[$Q$.pop()]
      \State $S$[$t$] := \Call{Compose}{left, right}
    \EndIf
    \State $Q$.push($t$)
  \EndFunction
\end{algorithmic}
\end{algorithm}

This stack representation requires substantially less space. It stores each element involved in the feedforward computation exactly once, meaning that this representation can still support efficient backpropagation. Furthermore, all of the updates to $S$ and $Q$ can be performed in-place on a GPU. These features allow this stack model to run efficiently on a GPU. We describe speed results in Section~\ref{sec:speed}.

\begin{table}
\centering
\begin{tabular}{c|cl}
  \toprule
  $t$ & $S$[$t$] & $Q_t$ \\
  \midrule
  0 & $a$ & 0 \\
  1 & $b$ & 0 1 \\
  2 & $c$ & 0 1 2 \\
  3 & $(c~b)$ & 0 3 \\
  4 & $((c~b)~a)$ & 4 \\
  \bottomrule
\end{tabular}
\caption{An example of the thin-stack algorithm computing a \shift-\shift-\shift-\reduce-\reduce~sequence on the input sentence $(a, b, c)$. $S$ is shown in the second column and may be thought of as a list of the tops of the stack at all timesteps $t$. The last two elements of $Q$ specify the rows $t$ which should be involved in a \reduce~operation at the next timestep.}
\label{tbl:thin-stack}
\end{table}

\subsection{TreeRNN-equivalence}

In its bare form, before the addition of the tracking LSTM, the SPINN (or, SPINN-NP-NT, for \textit{no parsing, no tracking}) is precisely equivalent to a conventional tree-structured neural network model in the function that it computes, and it therefore also has the same learning dynamics. In both, the representation of each sentence consists of the representation of the words, combined recursively using a TreeRNN composition function (in our case, the TreeLSTM function).

\section{Experiments}

\begin{table*}[t]
  \centering\small
  \begin{tabular}{lcccc} 
    \toprule
Model                   & Params.    & Trans. acc.  &   Train  &   Test \\
\midrule
\multicolumn{5}{c}{Previous non-NN results}\\
\midrule
Lexicalized classifier \citep{snli:emnlp2015}
                        & --                & --                    &   99.7   &   78.2      \\
\midrule
\multicolumn{5}{c}{Previous sentence encoder-based NN results}\\
\midrule
100d LSTM encoders \citep{snli:emnlp2015}
                        & 221k               & --               &   84.8   &   77.6      \\
1024d pretrained GRU encoders \citep{DBLP:journals/corr/VendrovKFU15}
                        & 15m                & --              &   98.8   &   81.4       \\
300d Tree-based CNN encoders \citep{mou2015recognizing}
                        & 3.5m                & --             &   83.4   &   82.1       \\
\midrule
\multicolumn{5}{c}{Our results}\\
\midrule
300d LSTM RNN encoders          & 3.0m                  & --                &   83.9      &   80.6       \\
300d SPINN-NP-NT encoders
                        & 3.4m                  & --                &   84.4      &   80.9       \\
300d SPINN-NP encoders
                        & 3.7m                  & --                &   89.2      &   \underline{83.2}       \\
\result{300d SPINN encoders }
                        & ??                  & 92.2+dv            &   ?    &   83.?      \\          
% \result{300d SPINN-NP, sequence-based attn.   }     
%                         & ?                  & --                &   ?      &   86.3dv         \\
% \result{300d SPINN-NP, tree-based attn. }           
%                         & ?                  & --                &   ?      &   \textbf{?}\\
\midrule
\multicolumn{5}{c}{Other previous NN results}\\
\midrule
100d LSTM w/ word-by-word attention \citep{rocktaschel2015reasoning}
                        & 252k               & --              &   85.3   &   83.5       \\
300d mLSTM word-by-word attention model \citep{DBLP:journals/corr/WangJ15b}
                        & 1.9m               & --             &   92.0   &   86.1      \\
300d LSTMN with deep attention fusion \citep{cheng2016long}
                        & 1.4m               & --                &   92.3   &   \textbf{89.1}      \\
    \bottomrule
  \end{tabular}
\caption{\protect\label{tab:results}Results on SNLI 3-way inference classification. Params. is the approximate number of trained parameters (excluding word embeddings for models where they are trained). Trans. acc. is the model's accuracy in predicting parsing transitions. Train and test are SNLI classification accuracy.} 
\end{table*}


\subsection{Natural language inference and SNLI}

We evaluate the SPINN on the task of natural language inference \citep[NLI, also known as recognizing textual entailment, or RTE][]{dagan2006pascal,MacCartney09}. NLI is a sentence pair classification task, in which a model reads two sentences (a premise and a hypothesis), and outputs a judgment of {\it entailment}, {\it contradiction}, or {\it neutral}, reflecting the relationship between the meanings of the two sentences, as in this example from the Stanford NLI corpus \citet[SNLI][]{snli:emnlp2015}), which we use for training and evaluation: 

\begin{quote}
Premise: {\it Girl in a red coat, blue head wrap and jeans is making a snow angel.}

Hypothesis: {\it A girl outside plays in the snow.}

Correct label: {\it entailment}
\end{quote}

Even though NLI is framed as a relatively simple three-way classification task, it is nonetheless an effective way of evaluating the ability of some model to extract broadly informative representations of sentence meaning. In order for a model to perform reliably well on NLI across a range of sentence types and text genres, it must be able to represent and reason with all of the core phenomena of natural language semantics, including quantification, coreference, scope, and several types of ambiguity.

SNLI is a corpus of 570k human-labeled pairs of scene descriptions like the one above. We use the standard train--test split and ignore unlabeled examples, which leaves about 549k examples for training, 9,842 for development, and 9,824 for testing. SNLI labels are roughly balanced, with the most frequent label, {\it entailment}, making up 34.2\% of the test set.


\subsection{Models evaluated} \todo{[SB] Condense.}

We evaluate four models on the SNLI task. Each uses 300d hidden states: \todo{[SB] d or D?}
\begin{itemize}
\item A baseline LSTM model (similar to that of \citet{snli:emnlp2015}) that uses the same classifier architecture as our models, but encodes sentences using a single layer LSTM-RNN sequence model.
\item The minimal SPINN-NP-NT (\textit{no parsing, no tracking}) model, which uses transitions from an external parser, and has no tracking LSTM to inform semantic composition. This model is equivalent to a TreeLSTM \citep{tai2015improved}.
\item The SPINN-NP model, which includes a tracking LSTM which guides composition, making it a hybrid tree-sequence model.
\item The full SPINN model, which makes its own parsing decisions, and does not depend on an external parser at test time, making it slightly weaker but a practical choice in applied settings.
\end{itemize}

We compare our models against a range of baselines, including the strongest published non-neural network-based result from \citep{snli:emnlp2015}, a set of previous neural network models built around sentence encoders, and a set of models built around soft attention architectures. We should note that these attention-based models are by far the strongest published on SNLI to date, but we do not wish to compare our model with those models directly. \todo{[SB] But we do. Rephrase. Move before naming of models.} SNLI was originally presented as an evaluation and development task for sentence encoding models, on the premise that sentence encoding models that do well on SNLI capture sentence meaning in a general sense, and are well suited to use on a range of NLP tasks. The attentional models are narrowly adapted to the sentence-pair structure of SNLI examples and cannot act as such general sentence encoders. We focus on models which do support general sentence encoding in our analysis.


\paragraph{Results} Table~\ref{tab:results} shows our results. Our primary evaluation metric for all models is three-way classification accuracy on the SNLI test set. The full SPINN model also generates parsing predictions, and we report a measure of agreement \todo{[SB] Clarify.} between this model's predictions and the Stanford Parser's predictions on the same data (calculated as classification accuracy over \shift~and \reduce~operations) \todo{[SB] Which Stanford parser.}. We also report the number of trained parameters in each model and training set accuracy.

We find that the bare SPINN-NP-NT model performs little better than the RNN baseline, but that the SPINN-NP with the added tracking LSTM reaches state-of-the-art accuracy among sentence encoding based models. The success of the SPINN-NP model, which is a hybrid tree-sequence model, suggests that the tree- and sequence-based encoding methods are complementary.

The full SPINN model with its relatively weak internal parser performs slightly less well, but nonetheless exceeds the performance of the LSTM baseline. None of our models reached the performance of the strongest attention-based models, which were explicitly engineered for the sentence pair-based structure of the SNLI task.

\todo{[SB] Discuss relationship between our model and the TBCNN. Add significance numbers.}

\subsection{Runtime performance}
\label{sec:speed}

\begin{figure}
\resizebox{\linewidth}{!}{
\begin{tikzpicture}
\begin{axis}[xmin=0, ymin=0, xlabel=Batch size, ylabel=Feedforward time (sec),
			 xtick={0,256,512,1024,2048}, legend style={at={(1.1, 0.98)},anchor=north east}]
  \addplot table [x={Batch size}, y=CPU, col sep=tab] {runtime.csv};
  \addlegendentry{CPU \citep{irsoy2014deep}}
  \addplot table [x={Batch size}, y=GPU, col sep=tab] {runtime.csv};
  \addlegendentry{Thin-stack GPU}
\end{axis}
\end{tikzpicture}
}
\caption{Runtime performance results with the thin-stack GPU algorithm.}
\label{fig:speed}
\end{figure}

We implemented the feedforward of the SPINN-NP-NT model (which is equivalent to a standard TreeRNN) in C++/CUDA in order to compare to the performance of a standard CPU-based TreeRNN solution from \citet{irsoy2014deep}. Such a standard TreeRNN implementation operates on a single example at a time and is thus not feasible to run on a GPU.\footnote{We chose to reimplement and evaluate only the thin-stack feedforward (not training/backpropagation) in C++/CUDA, as this is the more critical performance metric for real-world applications.}

Figure~\ref{fig:speed} compares the runtime performance of the two models computing representations on data from the Stanford Sentiment Treebank \todo{cite}. Each model is restricted to run on sentences of 30 tokens or fewer (for the SPINN model, this means we iterate over a sequence of 59 transitions). We fix the model dimensionality and word embedding dimensionality at 300 for all tests.

The CPU reference model is implemented in C++ and uses a single-core Eigen linear algebra backend. The implementation is a simplified version of the TreeRNN from \citet{irsoy2014deep}\footnote{The original code for this model is available at \url{https://github.com/oir/deep-recursive}. We plan to release our modified code.}, modified to exactly match the computation performed by our GPU model. We run the CPU performance test on a 2.20-GHz Intel Xeon processor with hyperthreading enabled. Our GPU model is implemented in C++ and CUDA, and will be released with the rest of our code. We test its performance on an NVIDIA Titan X GPU.

At batch sizes of interest for test-time applications (e.g., 512 or 1024 examples), we observe a 10--20x speedup relative to the standard CPU implementation. Figure~\ref{fig:speed} shows a substantial difference in runtime scaling as batch sizes increase, which is typical of GPU computation. We hope that this improved batchable TreeRNN algorithm will enable further rapid research in tree-structured neural networks.

\todo{Note that the full SPINN (i.e. Model 1) is in the same complexity class as SPINN-NP (what we're evaluating here) with very similar constants.}

\subsection{Parsing performance}

\todo{[AR] Fill in some details here.}
\todo{[SB] Read AR's report and work out what to include.}

\section{Discussion}

\question{What kind of error analysis would be interesting for this model?}

\result{[SB] Compare the types of errors in the different runs.}

\vspace{10em}

\section{Conclusions and future work}

We find that:
\begin{itemize}
\item Our SPINN-NP-NT model is exactly equivalent to a tree-structured network in what it computes, but is 10--20 times faster than a standard tree network at inference time.
\item While the use of tree-structure alone does not offer a substantial improvement over an LSTM baseline, our base tree-sequence hybrid model (SPINN-NP) substantially outperforms that baseline, suggesting that our model is able to successfully exploit tree structure, while using sequential context information to better resolve ambiguity.
\item It is possible to remove the dependence on an external parser from our model and replace it with a fast model-internal parser (as in the full SPINN) with only a small decrease in model performance.
\end{itemize}

\paragraph{Future work} Because this paper aims to introduce a general purpose model for sentence encoding, we did not pursue the use of soft attention, despite its demonstrated effectiveness on the SNLI task. However, the choice of whether to use soft attention and the choice of what representations to perform attention over are orthogonal \todo{[SB] Rephrase prev clause.}, and we expect that it should be possible to productively combine our model with soft attention to yield state-of-the-art performance.

The tracking LSTM that we incorporated into the SPINN uses only simple, quick-to-compute features drawn from the head of the buffer and the head of the stack. It is plausible that giving the tracking LSTM access to more information from the buffer and stack at each step would allow it to better represent the context of each tree node, including information about that nodes neighbors \todo{[SB] Rewrite.} in the sentence string and its likely position in the final tree, and that that context information could support both better parsing and better sentence encoding. One promising way to pursue this goal would be to encode the full contents of the stack and buffer at each time step following the method used by \citet{dyer-EtAl:2015:ACL-IJCNLP} for parsing.

For a more ambitious goal, we expect that it should be possible to implement a variant of the SPINN on top of a modified stack data structure with differentiable \textsc{push} and \textsc{pop} operations (possibly following \citet{grefenstette2015learning,joulin2015inferring}). This would make it possible for the model to learn to parse using guidance from the semantic representation objective, essentially allowing learn to produce parses that are, in aggregate, better suited to supporting semantic interpretation than those supplied in the training data. 

\todo{[SB] Fix run-ons here.}

%    \subsubsection*{Acknowledgments}
%    
%    Some of the Tesla K40(s) used for this research was/were donated by the NVIDIA Corporation.
%    \todo{[CM,CP] Acknowledge other grants.}
 
\bibliographystyle{../acl_natbib}
\bibliography{../MLSemantics}
\todo{[SB] Make bibliography style uniform, esp BatchNorm.}

 
\begin{table*}[ht]\small
\begin{center}
\begin{tabular}{lrlrrrr}
\toprule
Param.     & Range & Strategy        & RNN       & SPINN-NP-NT   & SPINN-NP  & SPINN \\
\midrule
Initial LR & 0.0002--0.02 & \textsc{log} & 0.005  & 0.0003 & 0.007  & SPINN\\
L2 regularization $\lambda$ & 8e-7--3e-5   & \textsc{log} & 4e-6  & 3e-6   & 2e-5  & SPINN\\
Transition cost $\alpha$  & 0.5--4.0 & \textsc{lin} & N/A & N/A & N/A  & SPINN\\
Embedding transformation dropout keep rate & 80\%--95\% & \textsc{lin} & N/A & 83\% & 92\%  & SPINN\\
Classifier MLP dropout keep rate & 80\%--95\% & \textsc{lin} & 94\%  & 94\%   & 93\%  & SPINN\\
Tracking LSTM size $D_t$ & 24--128 & \textsc{log} & N/A & N/A & 61  & SPINN\\
Classifier MLP layers & 1--3 & \textsc{lin} & 2 & 2 & 2 & SPINN\\
\bottomrule
\end{tabular}
\end{center}
 
\caption{
\label{tab:hyperparams}
Hyperparameter ranges and values. \ii{Range} shows the hyperparameter ranges explored during random search. \ii{Strategy} indicates whether sampling from the range was uniform, or log--uniform. \result{[SB] Full model hyperparams.}
}
\end{table*}

\appendix
\section{Hyperparameters}

Table \ref{tab:hyperparams} shows the hyperparameters used in the best run of each model.


\end{document}
